{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T23:21:13.774151Z",
     "start_time": "2021-08-24T23:21:13.757147Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "# import missingno\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.integrate import trapz\n",
    "\n",
    "# import missingno as msno\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('max_columns', 10, 'max_rows', 5, 'max_colwidth', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T23:22:14.931359Z",
     "start_time": "2021-08-24T23:22:13.916941Z"
    }
   },
   "outputs": [],
   "source": [
    "# 알고리즘(시계열)\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "import pmdarima as pm\n",
    "from pmdarima import model_selection\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Evaluation metrics\n",
    "# for regression\n",
    "from sklearn.metrics import mean_squared_log_error, mean_squared_error,  r2_score, mean_absolute_error\n",
    "\n",
    "# \n",
    "from matplotlib.patches import Patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시간컬럼 타입변경 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string 타입 datetime 타입으로 변경\n",
    "raw_all['datetime'] = pd.to_datetime(raw_all['datetime'])\n",
    "raw_all['DateTime'] = pd.to_datetime(raw_all['datetime'])\n",
    "\n",
    "# 시간 컬럼 인덱스로 설정\n",
    "raw_all.set_index('DateTime', inplace=True)\n",
    "raw_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_all.index\n",
    "raw_all.asfreq('D')\n",
    "raw_all.asfreq('W')\n",
    "raw_all.asfreq('H')\n",
    "raw_all.asfreq('H').isnull().sum()\n",
    "raw_all.asfreq('H')[raw_all.asfreq('H').isnull().sum(axis=1) > 0]\n",
    "raw_all.asfreq('H').head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_all = raw_all.asfreq('H', method='ffill')\n",
    "raw_all.isnull().sum()\n",
    "raw_all.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## y값 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y값 그래프로 확인\n",
    "raw_all[['count']].plot(kind='line', figsize=(20,6), linewidth=3, fontsize=20,\n",
    "                                              xlim=('2012-01-01', '2012-03-01'), ylim=(0,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data as trend + seasonal + residual 나눠서 확인\n",
    "plt.rcParams['figure.figsize'] = (14, 9)\n",
    "sm.tsa.seasonal_decompose(raw_all['count'], model='additive').plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data as trend * seasonal * residual 나눠서 확인\n",
    "sm.tsa.seasonal_decompose(raw_all['count'], model='multiplicative').plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill nan as some values of data\n",
    "result = sm.tsa.seasonal_decompose(raw_all['count'], model='additive')\n",
    "Y_trend = pd.DataFrame(result.trend)\n",
    "Y_trend.fillna(method='ffill', inplace=True)\n",
    "Y_trend.fillna(method='bfill', inplace=True)\n",
    "Y_trend.columns = ['count_trend']\n",
    "Y_trend.fillna(method='ffill', inplace=True)\n",
    "Y_trend.fillna(method='bfill', inplace=True)\n",
    "Y_trend.columns = ['count_trend']\n",
    "Y_seasonal = pd.DataFrame(result.seasonal)\n",
    "Y_seasonal.fillna(method='ffill', inplace=True)\n",
    "Y_seasonal.fillna(method='bfill', inplace=True)\n",
    "Y_seasonal.columns = ['count_seasonal']\n",
    "\n",
    "Y_seasonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정상성 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 정상성 테스트 및 모수추론(p=1, q=1, d=0, P=1, Q=1, D(m)=12)\n",
    "result = pd.Series(sm.tsa.stattools.adfuller(Y_train_feR.values.flatten())[0:4],\n",
    "                   index=['Test Statistics', 'p-value', 'Used Lag', 'Used Observations'])\n",
    "display(result)\n",
    "\n",
    "result = pd.Series(sm.tsa.stattools.kpss(Y_train_feR.values.flatten())[0:4],\n",
    "                   index=['Test Statistics', 'p-value', 'Used Lag', 'Used Observations'])\n",
    "display(result)\n",
    "\n",
    "plt.figure(figsize=(14,4))\n",
    "sm.tsa.graphics.plot_acf(Y_train_feR, lags=100, alpha=0.05, use_vlines=True, ax=plt.subplot(121))\n",
    "sm.tsa.graphics.plot_pacf(Y_train_feR, lags=100, alpha=0.05, use_vlines=True, ax=plt.subplot(122))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시간현실반영"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜에 2월 29일이 있는지 확인\n",
    "raw_fe.loc['2011-02-29', 'count_trend']\n",
    "\n",
    "# 2012-02-29??\n",
    "raw_fe.loc['2012-02-27':'2012-03-03', 'count_trend']\n",
    "raw_fe.loc['2012-02-28':'2012-03-01', 'count_trend']\n",
    "raw_fe.loc['2012-02-28 23:00:00', 'count_trend']\n",
    "raw_fe.loc['2012-03-01 00:00:00', 'count_trend']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date duplicattion by rules\n",
    "raw_fe.loc['2012-01-01':'2012-02-28', 'count_trend'] = raw_fe.loc['2011-01-01':'2011-02-28', 'count_trend'].values\n",
    "raw_fe.loc['2012-03-01':'2012-12-31', 'count_trend'] = raw_fe.loc['2011-03-01':'2011-12-31', 'count_trend'].values\n",
    "\n",
    "step = (raw_fe.loc['2011-03-01 00:00:00', 'count_trend'] - raw_fe.loc['2011-02-28 23:00:00', 'count_trend'])/25\n",
    "step_value = np.arange(raw_fe.loc['2011-02-28 23:00:00', 'count_trend']+step, \n",
    "                       raw_fe.loc['2011-03-01 00:00:00', 'count_trend'], step)\n",
    "step_value = step_value[:24]\n",
    "raw_fe.loc['2012-02-29', 'count_trend'] = step_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 스케일 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaler_fit = scaler.fit(X_train_feR)\n",
    "X_train_feRS = pd.DataFrame(scaler_fit.transform(X_train_feR), \n",
    "                                       index=X_train_feR.index, columns=X_train_feR.columns)\n",
    "X_test_feRS = pd.DataFrame(scaler_fit.transform(X_test_feR), \n",
    "                           index=X_test_feR.index, columns=X_test_feR.columns)\n",
    "X_test_feRS.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "scaler_fit = scaler.fit(X_train_feR)\n",
    "X_train_feRS = pd.DataFrame(scaler_fit.transform(X_train_feR), \n",
    "                                       index=X_train_feR.index, columns=X_train_feR.columns)\n",
    "X_test_feRS = pd.DataFrame(scaler_fit.transform(X_test_feR), \n",
    "                           index=X_test_feR.index, columns=X_test_feR.columns)\n",
    "X_test_feRS.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T00:47:43.108276Z",
     "start_time": "2021-08-25T00:47:43.105276Z"
    }
   },
   "source": [
    "### Log1p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_feRS = np.log1p(X_train_feR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.Normalizer()\n",
    "scaler_fit = scaler.fit(X_train_feR)\n",
    "X_train_feRS = pd.DataFrame(scaler_fit.transform(X_train_feR), \n",
    "                                       index=X_train_feR.index, columns=X_train_feR.columns)\n",
    "X_test_feRS = pd.DataFrame(scaler_fit.transform(X_test_feR), \n",
    "                           index=X_test_feR.index, columns=X_test_feR.columns)\n",
    "X_test_feRS.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다중공선성제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T01:04:35.959241Z",
     "start_time": "2021-08-25T01:04:35.949098Z"
    }
   },
   "source": [
    "### 수치로 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_feR.corr().loc[:, ['casual', 'registered', 'count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_feR.corr().loc[:, ['casual', 'registered', 'count']].style.background_gradient().set_precision(2).set_properties(**{'font-size': '11pt'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_feR.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시각화로 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in raw_feR.describe().columns:\n",
    "    target = raw_feR[col]\n",
    "    figure, axes = plt.subplots(2,1,figsize=(16,10))\n",
    "    sm.graphics.tsa.plot_acf(target, lags=100, use_vlines=True, ax=axes[0], title=col)\n",
    "    sm.graphics.tsa.plot_pacf(target, lags=100, use_vlines=True, ax=axes[1], title=col)  \n",
    "#     figure, axes = plt.subplots(1,1,figsize=(16,5))\n",
    "# #     sm.graphics.tsa.plot_acf(target, lags=100, use_vlines=True, ax=axes, title=col)\n",
    "#     sm.graphics.tsa.plot_pacf(target, lags=100, use_vlines=True, ax=axes, title=col)\n",
    "# count_trend, count_seasonal, count_Day, count_Week, count_diff, Hour, DayofWeek, count_lag1, count_lag2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T01:39:16.788105Z",
     "start_time": "2021-08-25T01:39:16.778103Z"
    }
   },
   "source": [
    "### VIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상관관계\n",
    "raw_feR.corr().loc[X_colname, X_colname].style.background_gradient().set_precision(2).set_properties(**{'font-size': '11pt'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract effective features using variance inflation factor\n",
    "vif = pd.DataFrame()\n",
    "vif['VIF_Factor'] = [variance_inflation_factor(X_train_feRS.values, i) \n",
    "                     for i in range(X_train_feRS.shape[1])]\n",
    "vif['Feature'] = X_train_feRS.columns\n",
    "vif.sort_values(by='VIF_Factor', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract effective features using variance inflation factor\n",
    "vif = pd.DataFrame()\n",
    "vif['VIF_Factor'] = [variance_inflation_factor(X_train_feRS.values, i) \n",
    "                     for i in range(X_train_feRS.shape[1])]\n",
    "vif['Feature'] = X_train_feRS.columns\n",
    "vif.sort_values(by='VIF_Factor', ascending=True)['Feature'][:10].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정상성 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a time-series\n",
    "raw_train = raw_fe.loc[raw_fe.index < '2012-07-01',:]\n",
    "raw_test = raw_fe.loc[raw_fe.index >= '2012-07-01',:]\n",
    "print(raw_train.shape, raw_test.shape)\n",
    "\n",
    "# data split of X and Y from train/test sets\n",
    "Y_train = raw_train[Y_colname]\n",
    "X_train = raw_train[X_colname]\n",
    "Y_test = raw_test[Y_colname]\n",
    "X_test = raw_test[X_colname]\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap_data = plt.cm.Paired\n",
    "cmap_cv = plt.cm.coolwarm\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "\n",
    "def plot_cv_indices(cv, X, n_splits, lw=10):\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "\n",
    "        # Visualize the results\n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n",
    "                   vmin=-.2, vmax=1.2)\n",
    "\n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits))\n",
    "    ax.set(yticks=np.arange(n_splits) + .5, yticklabels=yticklabels,\n",
    "           xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "           ylim=[n_splits+0.1, -.1], xlim=[0, len(X)])\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    \n",
    "    ax.legend([Patch(color=cmap_cv(.8)), Patch(color=cmap_cv(.02))],\n",
    "          ['Testing set', 'Training set'], loc=(1.02, .8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting with a simple array data\n",
    "XX = np.arange(100)\n",
    "n_split = 6\n",
    "tscv = TimeSeriesSplit(n_splits=n_split)\n",
    "plot_cv_indices(tscv, XX, n_splits=n_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockingTimeSeriesSplit():\n",
    "    def __init__(self, n_splits):\n",
    "        self.n_splits = n_splits\n",
    "        \n",
    "    def get_n_splits(self, groups):\n",
    "        return self.n_splits\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n_samples = len(X)\n",
    "        k_fold_size = n_samples // self.n_splits\n",
    "        indices = np.arange(n_samples)\n",
    "        \n",
    "        margin = 0\n",
    "        for i in range(self.n_splits):\n",
    "            start = i * k_fold_size\n",
    "            stop = start + k_fold_size\n",
    "            mid = int(0.8 * (stop - start)) + start\n",
    "            yield indices[start: mid], indices[mid + margin: stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btscv = BlockingTimeSeriesSplit(n_splits=n_split)\n",
    "plot_cv_indices(btscv, XX, n_splits=n_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T23:34:34.057136Z",
     "start_time": "2021-08-24T23:34:34.049134Z"
    }
   },
   "source": [
    "# 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearRegression (using statsmodels)\n",
    "fit_reg1 = sm.OLS(Y_train, X_train).fit()\n",
    "fit_reg1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 해석방식\n",
    "상단\n",
    "* Model : \n",
    "* Method :\n",
    "* R-squared : \n",
    "* Adj. R-squared : \n",
    "* F-statistic : \n",
    "* Prob (F-statistic) : \n",
    "* Log-likelihood :\n",
    "* AIC : \n",
    "* BIC : \n",
    "* No. Observations : \n",
    "\n",
    "하단\n",
    "* Omnibus : \n",
    "* Prob(Omnibus) :\n",
    "* Durbin-Watson :\n",
    "* Jaque-Bera (JB) :\n",
    "* Prob(JB) :\n",
    "* Skew :\n",
    "* Kutosis :\n",
    "* Cond. No. :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(fit_reg1.predict(X_train))\n",
    "# display(fit_reg1.predict(X_test))\n",
    "pred_tr_reg1 = fit_reg1.predict(X_train).values\n",
    "pred_te_reg1 = fit_reg1.predict(X_test).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 머신러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Regularization\n",
    "# Ridge\n",
    "fit_reg2 = Ridge(alpha=0.5, fit_intercept=True, normalize=False, random_state=123).fit(X_train_feRSM, Y_train_feR)\n",
    "pred_tr_reg2_feRSM = fit_reg2.predict(X_train_feRSM).flatten()\n",
    "pred_te_reg2_feRSM = fit_reg2.predict(X_test_feRSM).flatten()\n",
    "\n",
    "# Evaluation\n",
    "Score_reg2_feRSM, Resid_tr_reg2_feRSM, Resid_te_reg2_feRSM = evaluation_trte(Y_train_feR, pred_tr_reg2_feRSM,\n",
    "                                                                   Y_test_feR, pred_te_reg2_feRSM, graph_on=False)\n",
    "display(Score_reg2_feRSM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso\n",
    "fit_reg3 = Lasso(alpha=0.5, fit_intercept=True, normalize=False, random_state=123).fit(X_train_feRSM, Y_train_feR)\n",
    "pred_tr_reg3_feRSM = fit_reg3.predict(X_train_feRSM)\n",
    "pred_te_reg3_feRSM = fit_reg3.predict(X_test_feRSM)\n",
    "\n",
    "# Evaluation\n",
    "Score_reg3_feRSM, Resid_tr_reg3_feRSM, Resid_te_reg3_feRSM = evaluation_trte(Y_train_feR, pred_tr_reg3_feRSM,\n",
    "                                                                   Y_test_feR, pred_te_reg3_feRSM, graph_on=False)\n",
    "display(Score_reg3_feRSM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticNet\n",
    "fit_reg4 = ElasticNet(alpha=0.01, l1_ratio=1, fit_intercept=True, normalize=False, random_state=123).fit(X_train_feRSM, Y_train_feR)\n",
    "pred_tr_reg4_feRSM = fit_reg4.predict(X_train_feRSM)\n",
    "pred_te_reg4_feRSM = fit_reg4.predict(X_test_feRSM)\n",
    "\n",
    "# Evaluation\n",
    "Score_reg4_feRSM, Resid_tr_reg4_feRSM, Resid_te_reg4_feRSM = evaluation_trte(Y_train_feR, pred_tr_reg4_feRSM,\n",
    "                                                                   Y_test_feR, pred_te_reg4_feRSM, graph_on=False)\n",
    "display(Score_reg4_feRSM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bagging\n",
    "# DecisionTree\n",
    "fit_reg5 = DecisionTreeRegressor().fit(X_train_feRSM, Y_train_feR)\n",
    "pred_tr_reg5_feRSM = fit_reg5.predict(X_train_feRSM)\n",
    "pred_te_reg5_feRSM = fit_reg5.predict(X_test_feRSM)\n",
    "\n",
    "# Evaluation\n",
    "Score_reg5_feRSM, Resid_tr_reg5_feRSM, Resid_te_reg5_feRSM = evaluation_trte(Y_train_feR, pred_tr_reg5_feRSM,\n",
    "                                                                   Y_test_feR, pred_te_reg5_feRSM, graph_on=False)\n",
    "display(Score_reg5_feRSM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestRegressor\n",
    "fit_reg6 = RandomForestRegressor(n_estimators=100, random_state=123).fit(X_train_feRSM, Y_train_feR)\n",
    "pred_tr_reg6_feRSM = fit_reg6.predict(X_train_feRSM)\n",
    "pred_te_reg6_feRSM = fit_reg6.predict(X_test_feRSM)\n",
    "\n",
    "# Evaluation\n",
    "Score_reg6_feRSM, Resid_tr_reg6_feRSM, Resid_te_reg6_feRSM = evaluation_trte(Y_train_feR, pred_tr_reg6_feRSM,\n",
    "                                                                   Y_test_feR, pred_te_reg6_feRSM, graph_on=False)\n",
    "display(Score_reg6_feRSM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Effect of Variables\n",
    "Variable_Importances = pd.DataFrame([fit_reg6.feature_importances_], \n",
    "                                    columns=X_train_feRSM.columns, \n",
    "                                    index=['importance']).T.sort_values(by=['importance'], ascending=False)\n",
    "display(Variable_Importances)\n",
    "Variable_Importances.plot.bar(figsize=(12,6), fontsize=15)\n",
    "plt.title('Variable Importances', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GradientBoostingRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Boosting\n",
    "# GradientBoostingRegression\n",
    "fit_reg7 = GradientBoostingRegressor(alpha=0.1, learning_rate=0.05, loss='huber', criterion='friedman_mse',\n",
    "                                           n_estimators=1000, random_state=123).fit(X_train_feRSM, Y_train_feR)\n",
    "pred_tr_reg7_feRSM = fit_reg7.predict(X_train_feRSM)\n",
    "pred_te_reg7_feRSM = fit_reg7.predict(X_test_feRSM)\n",
    "\n",
    "# Evaluation\n",
    "Score_reg7_feRSM, Resid_tr_reg7_feRSM, Resid_te_reg7_feRSM = evaluation_trte(Y_train_feR, pred_tr_reg7_feRSM,\n",
    "                                                                   Y_test_feR, pred_te_reg7_feRSM, graph_on=False)\n",
    "display(Score_reg7_feRSM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "fit_reg8 = XGBRegressor(learning_rate=0.05, n_estimators=100, random_state=123).fit(X_train_feRSM, Y_train_feR)\n",
    "pred_tr_reg8_feRSM = fit_reg8.predict(X_train_feRSM)\n",
    "pred_te_reg8_feRSM = fit_reg8.predict(X_test_feRSM)\n",
    "\n",
    "# Evaluation\n",
    "Score_reg8_feRSM, Resid_tr_reg8_feRSM, Resid_te_reg8_feRSM = evaluation_trte(Y_train_feR, pred_tr_reg8_feRSM,\n",
    "                                                                   Y_test_feR, pred_te_reg8_feRSM, graph_on=False)\n",
    "display(Score_reg8_feRSM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGMB\n",
    "fit_reg9 = LGBMRegressor(learning_rate=0.05, n_estimators=100, random_state=123).fit(X_train_feRSM, Y_train_feR)\n",
    "pred_tr_reg9_feRSM = fit_reg9.predict(X_train_feRSM)\n",
    "pred_te_reg9_feRSM = fit_reg9.predict(X_test_feRSM)\n",
    "\n",
    "# Evaluation\n",
    "Score_reg9_feRSM, Resid_tr_reg9_feRSM, Resid_te_reg9_feRSM = evaluation_trte(Y_train_feR, pred_tr_reg9_feRSM,\n",
    "                                                                   Y_test_feR, pred_te_reg9_feRSM, graph_on=False)\n",
    "display(Score_reg9_feRSM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T00:28:20.310117Z",
     "start_time": "2021-08-25T00:28:20.304116Z"
    }
   },
   "source": [
    "## AR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T00:28:20.618974Z",
     "start_time": "2021-08-25T00:28:20.607972Z"
    }
   },
   "source": [
    "## MA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pmdarima as pm\n",
    "from pmdarima import model_selection\n",
    "\n",
    "# Data Loading and Split\n",
    "data = pm.datasets.load_wineind()\n",
    "train, test = model_selection.train_test_split(data, train_size=150)\n",
    "\n",
    "# 모델링\n",
    "autoarima = pm.auto_arima(train,  \n",
    "                          stationary=False,\n",
    "                          with_intercept=True,\n",
    "#                           start_p=0, d=None, start_q=0,\n",
    "#                           max_p=5, max_d=1, max_q=5,\n",
    "                          seasonal=True, m=12,\n",
    "#                           start_P=0, D=None, start_Q=0,\n",
    "#                           max_P=5, max_D=1, max_Q=5,\n",
    "                          max_order=30, maxiter=5,\n",
    "                          information_criterion='bic',\n",
    "                          trace=True, suppress_warnings=True)\n",
    "display(autoarima.summary())\n",
    "pred_tr_ts_autoarima = autoarima.predict_in_sample()\n",
    "pred_tr_ts_autoarima = autoarima.predict(n_periods=len(train))\n",
    "pred_te_ts_autoarima = autoarima.predict(n_periods=len(test), \n",
    "                                         return_conf_int=True)[0]\n",
    "pred_te_ts_autoarima_ci = autoarima.predict(n_periods=len(test), \n",
    "                                            return_conf_int=True)[1]\n",
    "\n",
    "\n",
    "\n",
    "# 잔차진단\n",
    "error_analysis(Resid_tr_ts_autoarima, ['Error'], pd.DataFrame(train), graph_on=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증\n",
    "Score_ts_autoarima, Resid_tr_ts_autoarima, Resid_te_ts_autoarima = evaluation_trte(pd.DataFrame(train), pred_tr_ts_autoarima, \n",
    "                                                                                   pd.DataFrame(test), pred_te_ts_autoarima, graph_on=True)\n",
    "display(Score_ts_autoarima)\n",
    "ax = pd.DataFrame(test).plot(figsize=(12,4))\n",
    "pd.DataFrame(pred_te_ts_autoarima, columns=['prediction']).plot(kind='line',\n",
    "                                                                linewidth=3, fontsize=20, ax=ax)\n",
    "ax.fill_between(pd.DataFrame(pred_te_ts_autoarima_ci).index,\n",
    "                pd.DataFrame(pred_te_ts_autoarima_ci).iloc[:,0],\n",
    "                pd.DataFrame(pred_te_ts_autoarima_ci).iloc[:,1], color='k', alpha=0.15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 정상성 변화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SARIMA\n",
    "fit_ts_sarimax = sm.tsa.SARIMAX(Y_train_feR, trend='c', order=(1,trend_diff_order,1), \n",
    "                                seasonal_order=(1,seasonal_diff_order,1,seasonal_order)).fit()\n",
    "pred_tr_ts_sarimax = fit_ts_sarimax.predict()\n",
    "pred_te_ts_sarimax = fit_ts_sarimax.get_forecast(len(Y_test_feR)).predicted_mean\n",
    "pred_te_ts_sarimax_ci = fit_ts_sarimax.get_forecast(len(Y_test_feR)).conf_int()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증\n",
    "Score_ts_sarimax, Resid_tr_ts_sarimax, Resid_te_ts_sarimax = evaluation_trte(Y_train_feR, pred_tr_ts_sarimax, \n",
    "                                                                             Y_test_feR, pred_te_ts_sarimax, graph_on=True)\n",
    "display(Score_ts_sarimax)\n",
    "ax = pd.DataFrame(Y_test_feR).plot(figsize=(12,4))\n",
    "pd.DataFrame(pred_te_ts_sarimax, index=Y_test_feR.index, columns=['prediction']).plot(kind='line',\n",
    "                                                                           xlim=(Y_test_feR.index.min(),Y_test_feR.index.max()),\n",
    "                                                                           linewidth=3, fontsize=20, ax=ax)\n",
    "ax.fill_between(pd.DataFrame(pred_te_ts_sarimax_ci, index=Y_test_feR.index).index,\n",
    "                pd.DataFrame(pred_te_ts_sarimax_ci, index=Y_test_feR.index).iloc[:,0],\n",
    "                pd.DataFrame(pred_te_ts_sarimax_ci, index=Y_test_feR.index).iloc[:,1], color='k', alpha=0.15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 성능 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시각화 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측\n",
    "pd.concat([Y_train, pd.DataFrame(pred_tr_reg1, index=Y_train.index, columns=['prediction'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 그래프로 보기\n",
    "pd.concat([Y_train, pd.DataFrame(pred_tr_reg1, index=Y_train.index, columns=['prediction'])], axis=1).plot(kind='line', figsize=(20,6),\n",
    "                                                                                                               xlim=(Y_train.index.min(),Y_train.index.max()),\n",
    "                                                                                                               linewidth=0.5, fontsize=20)\n",
    "plt.title('Time Series of Target', fontsize=20)\n",
    "plt.xlabel('Index', fontsize=15)\n",
    "plt.ylabel('Target Value', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 수치 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE = abs(Y_train.values.flatten() - pred_tr_reg1).mean()\n",
    "MSE = ((Y_train.values.flatten() - pred_tr_reg1)**2).mean()\n",
    "MAPE = (abs(Y_train.values.flatten() - pred_tr_reg1)/Y_train.values.flatten()*100).mean()\n",
    "pd.DataFrame([MAE, MSE, MAPE], index=['MAE', 'MSE', 'MAPE'], columns=['Score']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 잔차 진단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Resid_tr_reg1['RowNum'] = Resid_tr_reg1.reset_index().index\n",
    "Resid_tr_reg1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T23:53:15.879449Z",
     "start_time": "2021-08-24T23:53:15.866445Z"
    }
   },
   "source": [
    "## 수치 검증"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functionalize\n",
    "### Error analysis\n",
    "def stationarity_adf_test(Y_Data, Target_name):\n",
    "    if len(Target_name) == 0:\n",
    "        Stationarity_adf = pd.Series(sm.tsa.stattools.adfuller(Y_Data)[0:4],\n",
    "                                     index=['Test Statistics', 'p-value', 'Used Lag', 'Used Observations'])\n",
    "        for key, value in sm.tsa.stattools.adfuller(Y_Data)[4].items():\n",
    "            Stationarity_adf['Critical Value(%s)'%key] = value\n",
    "            Stationarity_adf['Maximum Information Criteria'] = sm.tsa.stattools.adfuller(Y_Data)[5]\n",
    "            Stationarity_adf = pd.DataFrame(Stationarity_adf, columns=['Stationarity_adf'])\n",
    "    else:\n",
    "        Stationarity_adf = pd.Series(sm.tsa.stattools.adfuller(Y_Data[Target_name])[0:4],\n",
    "                                     index=['Test Statistics', 'p-value', 'Used Lag', 'Used Observations'])\n",
    "        for key, value in sm.tsa.stattools.adfuller(Y_Data[Target_name])[4].items():\n",
    "            Stationarity_adf['Critical Value(%s)'%key] = value\n",
    "            Stationarity_adf['Maximum Information Criteria'] = sm.tsa.stattools.adfuller(Y_Data[Target_name])[5]\n",
    "            Stationarity_adf = pd.DataFrame(Stationarity_adf, columns=['Stationarity_adf'])\n",
    "    return Stationarity_adf\n",
    "\n",
    "def stationarity_kpss_test(Y_Data, Target_name):\n",
    "    if len(Target_name) == 0:\n",
    "        Stationarity_kpss = pd.Series(sm.tsa.stattools.kpss(Y_Data)[0:3],\n",
    "                                      index=['Test Statistics', 'p-value', 'Used Lag'])\n",
    "        for key, value in sm.tsa.stattools.kpss(Y_Data)[3].items():\n",
    "            Stationarity_kpss['Critical Value(%s)'%key] = value\n",
    "            Stationarity_kpss = pd.DataFrame(Stationarity_kpss, columns=['Stationarity_kpss'])\n",
    "    else:\n",
    "        Stationarity_kpss = pd.Series(sm.tsa.stattools.kpss(Y_Data[Target_name])[0:3],\n",
    "                                      index=['Test Statistics', 'p-value', 'Used Lag'])\n",
    "        for key, value in sm.tsa.stattools.kpss(Y_Data[Target_name])[3].items():\n",
    "            Stationarity_kpss['Critical Value(%s)'%key] = value\n",
    "            Stationarity_kpss = pd.DataFrame(Stationarity_kpss, columns=['Stationarity_kpss'])\n",
    "    return Stationarity_kpss\n",
    "\n",
    "def error_analysis(Y_Data, Target_name, X_Data, graph_on=False):\n",
    "    for x in Target_name:\n",
    "        Target_name = x\n",
    "    X_Data = X_Data.loc[Y_Data.index]\n",
    "\n",
    "    if graph_on == True:\n",
    "        ##### Error Analysis(Plot)\n",
    "        Y_Data['RowNum'] = Y_Data.reset_index().index\n",
    "\n",
    "        # Stationarity(Trend) Analysis\n",
    "        sns.set(palette=\"muted\", color_codes=True, font_scale=2)\n",
    "        sns.lmplot(x='RowNum', y=Target_name, data=Y_Data, fit_reg='True', size=5.2, aspect=2, ci=99, sharey=True)\n",
    "        del Y_Data['RowNum']\n",
    "\n",
    "        # Normal Distribution Analysis\n",
    "        figure, axes = plt.subplots(figsize=(12,8))\n",
    "        sns.distplot(Y_Data[Target_name], norm_hist='True', fit=stats.norm, ax=axes)\n",
    "\n",
    "        # Lag Analysis\n",
    "        length = int(len(Y_Data[Target_name])/10)\n",
    "        figure, axes = plt.subplots(1, 4, figsize=(12,3))\n",
    "        pd.plotting.lag_plot(Y_Data[Target_name], lag=1, ax=axes[0])\n",
    "        pd.plotting.lag_plot(Y_Data[Target_name], lag=5, ax=axes[1])\n",
    "        pd.plotting.lag_plot(Y_Data[Target_name], lag=10, ax=axes[2])\n",
    "        pd.plotting.lag_plot(Y_Data[Target_name], lag=50, ax=axes[3])\n",
    "\n",
    "        # Autocorrelation Analysis\n",
    "        figure, axes = plt.subplots(2,1,figsize=(12,5))\n",
    "        sm.tsa.graphics.plot_acf(Y_Data[Target_name], lags=100, use_vlines=True, ax=axes[0])\n",
    "        sm.tsa.graphics.plot_pacf(Y_Data[Target_name], lags=100, use_vlines=True, ax=axes[1])\n",
    "\n",
    "    ##### Error Analysis(Statistics)\n",
    "    # Checking Stationarity\n",
    "    # Null Hypothesis: The Time-series is non-stationalry\n",
    "    Stationarity_adf = stationarity_adf_test(Y_Data, Target_name)\n",
    "    Stationarity_kpss = stationarity_kpss_test(Y_Data, Target_name)\n",
    "\n",
    "    # Checking of Normality\n",
    "    # Null Hypothesis: The residuals are normally distributed\n",
    "    Normality = pd.DataFrame([stats.shapiro(Y_Data[Target_name])],\n",
    "                             index=['Normality'], columns=['Test Statistics', 'p-value']).T\n",
    "\n",
    "    # Checking for Autocorrelation\n",
    "    # Null Hypothesis: Autocorrelation is absent\n",
    "    Autocorrelation = pd.concat([pd.DataFrame(sm.stats.diagnostic.acorr_ljungbox(Y_Data[Target_name], lags=[1,5,10,50])[0], columns=['Test Statistics']),\n",
    "                                 pd.DataFrame(sm.stats.diagnostic.acorr_ljungbox(Y_Data[Target_name], lags=[1,5,10,50])[1], columns=['p-value'])], axis=1).T\n",
    "    Autocorrelation.columns = ['Autocorr(lag1)', 'Autocorr(lag5)', 'Autocorr(lag10)', 'Autocorr(lag50)']\n",
    "\n",
    "    # Checking Heteroscedasticity\n",
    "    # Null Hypothesis: Error terms are homoscedastic\n",
    "    Heteroscedasticity = pd.DataFrame([sm.stats.diagnostic.het_goldfeldquandt(Y_Data[Target_name], X_Data.values, alternative='two-sided')],\n",
    "                                      index=['Heteroscedasticity'], columns=['Test Statistics', 'p-value', 'Alternative']).T\n",
    "    Score = pd.concat([Stationarity_adf, Stationarity_kpss, Normality, Autocorrelation, Heteroscedasticity], join='outer', axis=1)\n",
    "    index_new = ['Test Statistics', 'p-value', 'Alternative', 'Used Lag', 'Used Observations',\n",
    "                 'Critical Value(1%)', 'Critical Value(5%)', 'Critical Value(10%)', 'Maximum Information Criteria']\n",
    "    Score.reindex(index_new)\n",
    "    return Score\n",
    "# error_analysis(Resid_tr_reg1[1:], ['Error'], X_train, graph_on=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis(Resid_tr_reg1, ['Error'], X_train, graph_on=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 개별"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Analysis(Statistics)\n",
    "# Checking Stationarity\n",
    "# ADF(Null Hypothesis: The Time-series is non-stationalry)\n",
    "Stationarity_adf = pd.Series(sm.tsa.stattools.adfuller(Resid_tr_reg1['Error'])[0:4], index=['Test Statistics', 'p-value', 'Used Lag', 'Used Observations'])\n",
    "for key, value in sm.tsa.stattools.adfuller(Resid_tr_reg1['Error'])[4].items():\n",
    "    Stationarity['Critical Value(%s)'%key] = value\n",
    "Stationarity['Maximum Information Criteria'] = sm.tsa.stattools.adfuller(Resid_tr_reg1['Error'])[5]\n",
    "Stationarity = pd.DataFrame(Stationarity, columns=['Stationarity_adf'])\n",
    "\n",
    "# KPSS(Null Hypothesis: The Time-series is non-stationalry)\n",
    "Stationarity_kpss = pd.Series(sm.tsa.stattools.kpss(Resid_tr_reg1['Error'])[0:4], index=['Test Statistics', 'p-value', 'Used Lag', 'Used Observations'])\n",
    "for key, value in sm.tsa.stattools.kpss(Resid_tr_reg1['Error'])[4].items():\n",
    "    Stationarity['Critical Value(%s)'%key] = value\n",
    "Stationarity['Maximum Information Criteria'] = sm.tsa.stattools.kpss(Resid_tr_reg1['Error'])[5]\n",
    "Stationarity = pd.DataFrame(Stationarity, columns=['Stationarity_kpss'])\n",
    "\n",
    "# Checking of Normality\n",
    "# Null Hypothesis: The residuals are normally distributed\n",
    "Normality = pd.DataFrame([stats.shapiro(Resid_tr_reg1['Error'])], index=['Normality'], columns=['Test Statistics', 'p-value']).T\n",
    "\n",
    "# Checking for Autocorrelation\n",
    "# Null Hypothesis: Autocorrelation is absent\n",
    "Autocorrelation = pd.concat([pd.DataFrame(sm.stats.diagnostic.acorr_ljungbox(Resid_tr_reg1['Error'], lags=[1,5,10,50])[0], columns=['Test Statistics']),\n",
    "                             pd.DataFrame(sm.stats.diagnostic.acorr_ljungbox(Resid_tr_reg1['Error'], lags=[1,5,10,50])[1], columns=['p-value'])], axis=1).T\n",
    "Autocorrelation.columns = ['Autocorr(lag1)', 'Autocorr(lag5)', 'Autocorr(lag10)', 'Autocorr(lag50)']\n",
    "\n",
    "# Checking Heteroscedasticity\n",
    "# Null Hypothesis: Error terms are homoscedastic\n",
    "Heteroscedasticity = pd.DataFrame([sm.stats.diagnostic.het_goldfeldquandt(Resid_tr_reg1['Error'], X_train.values, alternative='two-sided')],\n",
    "                                  index=['Heteroscedasticity'], columns=['Test Statistics', 'p-value', 'Alternative']).T\n",
    "Error_Analysis = pd.concat([Stationarity, Normality, Autocorrelation, Heteroscedasticity], join='outer', axis=1)\n",
    "Error_Analysis = Error_Analysis.loc[['Test Statistics', 'p-value', 'Alternative', 'Used Lag', 'Used Observations',\n",
    "                                     'Critical Value(1%)', 'Critical Value(5%)', 'Critical Value(10%)',\n",
    "                                     'Maximum Information Criteria'],:]\n",
    "Error_Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 해석 방식\n",
    "* Stationarity :\n",
    "* Normality :\n",
    "* Autocorr(lag n) :\n",
    "* Heteroscedasticity : 등분산 검증으로 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시각화 검증"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 화이트 노이즈 확인(트랜드, 계절성 포함 여부)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(palette=\"muted\", color_codes=True, font_scale=2)\n",
    "sns.lmplot(data=Resid_tr_reg1.iloc[1:], x='RowNum', y='Error',\n",
    "           fit_reg=True, line_kws={'color': 'red'}, size=5.2, aspect=2, ci=99, sharey=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정규성 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(Resid_tr_reg1['Error'].iloc[1:], norm_hist='True', fit=stats.norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lag 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag Analysis\n",
    "length = int(len(Resid_tr_reg1['Error'])/10)\n",
    "figure, axes = plt.subplots(1, 4, figsize=(12,3))\n",
    "pd.plotting.lag_plot(Resid_tr_reg1['Error'], lag=1, ax=axes[0])\n",
    "pd.plotting.lag_plot(Resid_tr_reg1['Error'], lag=5, ax=axes[1])\n",
    "pd.plotting.lag_plot(Resid_tr_reg1['Error'], lag=10, ax=axes[2])\n",
    "pd.plotting.lag_plot(Resid_tr_reg1['Error'], lag=50, ax=axes[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자기상관 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autocorrelation Analysis\n",
    "figure, axes = plt.subplots(2,1,figsize=(12,5))\n",
    "figure = sm.graphics.tsa.plot_acf(Resid_tr_reg1['Error'].iloc[1:], lags=100, use_vlines=True, ax=axes[0])\n",
    "figure = sm.graphics.tsa.plot_pacf(Resid_tr_reg1['Error'].iloc[1:], lags=100, use_vlines=True, ax=axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 등분산 검증\n",
    "수치로만 확인함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "255.857px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
